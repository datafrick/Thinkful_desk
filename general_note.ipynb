{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENCH TESTING SPACE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('purchases.csv')\n",
    "#print(df)\n",
    "useful_string = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r']\n",
    "np.array(useful_string).reshape(6,3)\n",
    "#print(df)\n",
    "df['test_data'] = ['saheed', 'tijani', 'adebayo', 'saheed', 'tijani', 'adebayo', 'saheed', 'tijani', 'adebayo', 'saheed']\n",
    "#print(df)\n",
    "#df.to_csv('my_data.csv')\n",
    "\n",
    "print(np.random.randn(140,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL PYTHON HINTS\n",
    "\n",
    "*Opening and reading a file;\n",
    "\n",
    "with open('poem.txt') as poem_file:\n",
    "    text = poem_file.readlines()\n",
    "    print(\"This file is {} lines long\".format(len(text)))\n",
    "    for line in text:\n",
    "        print(line)\n",
    "        \n",
    "*Opening a file with open() will leave it open until you close it. The .close() file object method will close a file. It's super easy to forget to manually close files, which can keep resources tied up and cause unexpected trouble. Luckily, Python gives us the with statement you see used above so we don't have to remember to use .close(), because files opened in a with statement will automatically be closed once the with statement exits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUPYTER HINTS\n",
    "\n",
    "*lunch jupyter; jupyter notebook [your particular/file path], this opens an ongoing job\n",
    "*Convert cell to a markdown; activate the cell and type \"m\".\n",
    "*To run an individual cell, \"shift + enter\" from within the active cell\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMPY HINTS\n",
    "\n",
    "*basically for doing more advance mathematics and storing data in analytic friendly way, close to MATLAB, IMO\n",
    "\n",
    "*creating an array; x = np.array([0, 1, 2, 3])\n",
    "\n",
    "*creating multidimentional arrays; y = np.arange(8).reshape(2, 4) = ([[0, 1, 2, 3],[4, 5, 6, 7]]) or \n",
    "    manually; w = np.array([[0, 1, 2, 3],[4, 5, 6, 7]]), arange is the short for \"array range\". \n",
    "    \n",
    "*E.g of arange() useful_string = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r']\n",
    "    np.array(useful_string).reshape(6,3)\n",
    "    \n",
    "*Useful numpy aggregator methods: print(np.max(x)), print(np.std(x)), print(np.mean(x)).\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS HINT\n",
    "\n",
    "*df = pd.DataFrame(my_array), np could be used to create the array\n",
    "\n",
    "*words = pd.Series([12,'eed', 10]), creates a series with index\n",
    "\n",
    "*column are identified by column names while rows are indexed from 0 by default. Rows can also be strings using the index keyword columns=['Capital', 'Nickname','Area'], index=['Alabama', 'California', 'Oklahoma']):\n",
    "\n",
    "*purchases = pd.DataFrame(index=names), creates an empty df with the elements of 'names' as index\n",
    "\n",
    "*purchases['country'] = ['US', 'CAN', 'CAN', 'US', 'CAN', 'US', 'US', 'US', 'CAN', 'US'], create column 'country'\n",
    "\n",
    "*\"df.column_name\" or df['column_name'] to get column 'column_name', the later is most preffered\n",
    "\n",
    "*purchases['items_purch_per_ad'] = purchases['items_purchased'] / purchases['ad_views']\n",
    "\n",
    "*purchases.loc['index_name'] to select all the elements in the row\n",
    "\n",
    "*purchases.loc[:, 'column_name'],to select all the element in the column\n",
    "\n",
    "*purchases.loc['row_name', 'column_name'], to select a single element\n",
    "\n",
    "*purchases.iloc[1:3, 1], select the first elements in row1:col1, row2:col1\n",
    "\n",
    "*df['counts_sq'] = df.index ** 2, manipulating index to create another column\n",
    "\n",
    "*data.head(), top contents\n",
    "\n",
    "*counttable = pd.crosstab(df_2cat['cntry'], df_2cat['partner_cat']), generate counttable\n",
    "\n",
    "#create a new column with empty rows\n",
    "df['partner_cat'] = None\n",
    "\n",
    "#locate and replace. for the 'partner' col, where value is 1, replace value by 'Lives with partner'\n",
    "df.loc[df['partner'] == 1, 'partner_cat'] = 'Lives with partner'\n",
    "\n",
    "#filtering data: https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/\n",
    "df_czch = df.loc[((df['cntry'] == 'CZ') | (df['cntry'] == 'CH')) & (df['year'] == 6),['cntry', 'tvtot', 'ppltrst', 'pplfair', 'pplhlp', 'happy', 'sclmeet']]\n",
    "\n",
    "#dropna\n",
    "df.dropna()\n",
    "\n",
    "#get the dimension of the df\n",
    "df.shape\n",
    "\n",
    "#adding two df together, they must have similar column titles and shapes\n",
    "df = df1.add(df2)\n",
    "\n",
    "#to add the col titles of df1 to df2\n",
    "jitter = pd.DataFrame(np.random.uniform(-.3, .3, size=(df_jittered.shape)),columns=df_jittered.columns)\n",
    "\n",
    "*T-TEST AND P-TEST\n",
    "\n",
    "#multilevel indexing\n",
    "data_test = data.set_index(['groups','size','variability'])\n",
    "#create subgroup for each group based on the multilevel indices\n",
    "a = data_test['data'].xs(('group1',size,var),level=('groups','size','variability'))\n",
    "#return t-test and p-vaue for two data sets or groups\n",
    "tval,pval=stats.ttest_ind(b, a,equal_var=True)\n",
    "#basically in data science, a p-value less that 0.05 is considered good as it negates the null argument, while any \n",
    "#p-value greater 5%(0.05) is considered bad as it's seen as tending too much towards the null argument. E.g, people\n",
    "#pray are likely to live happier lives - hypothesis, the null argument would be, people who pray are not likey to live\n",
    "#happier than those who do not.\n",
    "\n",
    "#The typical interpretation of p-values (\"less than .05 means my hypothesis was right, yay!\") works fairly well in \n",
    "#practical terms. However, it's important to know that interpretation is wrong. The p-value represents the probability \n",
    "#of getting the data you have if the null hypothesis were true in the population. \n",
    "\n",
    "*DATA DSCRIPTION: \n",
    "\n",
    "*[df.groupby('gender').describe()], group and describe\n",
    "*[data.gender.value_counts()], count through column\n",
    "\n",
    "*LAMBDA: a construction that allows for defining anonymous function at run time. it's used to create a consition on the row or column. purchases.loc[lambda df: purchases['items_purchased'] > 1, :], for any row whhose item purchased cell is >1, extact all it's column. df is used as this fuction takes a dataframe as input. A simpler way is using; purchases[purchases['items_purchased'] > 1]\n",
    "\n",
    "*purchases.groupby('country').aggregate(np.mean), it groups all the column by countries and take average of the other corresponding grouped columns.\n",
    "\n",
    "*LOAD FILE: df = pd.read_csv('purchases.csv'), http://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files; for further options on how to load files\n",
    "\n",
    "\n",
    "*WRITING TO FILE: df.to_csv('my_data.csv'), http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\n",
    "\n",
    "*JSON stands for \"JavaScript Object Notation\", it's semi structured, it's a way to represent java script object as a sting, it's like key value pair in python. Jason file are human readable like CSV and can be opened with text editors.\n",
    "\n",
    "*LOAD FILEdf = pd.read_json('purchases.json'), to import a file. http://pandas.pydata.org/pandas-docs/stable/io.html#json\n",
    "\n",
    "*WRITTING FILE: df.to_json('my_data.json')\n",
    "\n",
    "*The more common use case is to send JSON data over the web. For that we'd call 'to_json()' without a path argument to create a JSON string that we'd later process and send: serialized_purchases = df.to_json()\n",
    "\n",
    "*XML, or \"eXtensible Markup Language\", like JSON, is a hierarchical semi-structured data format, but the newer json is more common these days.Pandas doesn't have an XML equivalent to read_csv() and read_json, so we'll use the xml module from the Python standard library to read in XML files and convert them to an element tree. Once we have an element tree we'll manually process it into a list that we can feed into Pandas.\n",
    "\n",
    "*means = df[['ppltrst','pplfair','pplhlp']].mean(axis=0), takes means of each col, returns 3 values\n",
    "\n",
    "*means = df[['ppltrst','pplfair','pplhlp']].mean(axis=1), takes means accross each row, returns len(df) values\n",
    "\n",
    "*stds = df[['ppltrst','pplfair','pplhlp']].std(axis=0), same for mean\n",
    "\n",
    "*https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/, hints on dropping rows and col.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a1ab14b5d724>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-a1ab14b5d724>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    *matplotlib is the standard plotting library for python and another part of the Scientific Python Toolkit.\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# DATA VISUALISATION WITH MATPLOTLIB HINTS\n",
    "\n",
    "*matplotlib is the standard plotting library for python and another part of the Scientific Python Toolkit. \n",
    "\n",
    "*it is actually the foundation for many of the other plotting packages in Python, pyplot is a portion of the library\n",
    "\n",
    "*import matplotlib.pyplot as plt\n",
    "\n",
    "*If you're using Jupyter notebooks you'll want to use the magic %matplotlib inline to get your plots rendering well. It's good practice to always include that line in your first cell when working with Jupyter notebooks, along with loading your packages. This statement makes sure your images will always generate in the notebook instead of possibly as popups and prevents a slow matplotlib compilation from giving you rendering errors.\n",
    "\n",
    "LINE CHARTS\n",
    "\n",
    "*plt.plot([0, 1, 2, 3]), plt.show(). thesse two lines plot a straigh line gragh from the origin\n",
    "\n",
    "*plt.plot(df['rand']), plot a column of a dataframe\n",
    "\n",
    "*By default, plot creates a line graph with the index as the x-axis. Recall that by default the index is a count starting at 0.\n",
    "\n",
    "*plt.plot(df['rand'], color='purple')\n",
    "\n",
    "*state a color, plt.ylim([-0.1, 1.1]), axis limit\n",
    "\n",
    "*plt.ylabel('Values'), axis label\n",
    "\n",
    "*plt.title('Random Series'), plot title\n",
    "\n",
    "*tadd a second plot on the same axis, call plot a second time for the second series. plt.plot(df['rand_shift'], color='green')\n",
    "\n",
    "SCATTER\n",
    "\n",
    "*This works much the same as .plot() and is called with .scatter() instead.\n",
    "\n",
    "*plt.scatter(x=df['rand'], y=df['rand_sq']) or with more options;\n",
    "\n",
    "*plt.scatter(x=df['rand'], y=df['rand_sq'], color='purple',marker='x', s=10)\n",
    "\n",
    "SUBPLOTS\n",
    "\n",
    "*Subplots are matplotlib's way of generating multiple plots in a single figure.\n",
    "*To do that, we need to define the subplot before generating each plot. That can be done using plt.subplot(), \n",
    "which takes three arguments. The first two parameters define the dimensions of the plot, while the third identifies the \n",
    "subplot you're creating\n",
    "*plt.figure(figsize=(10, 5)), specifies the x and y dimension of the whole plot area.\n",
    "*plt.subplot(1, 2, 1), the first 2 parameter set the grid(x,y), the 3rd indicate the plot that will be inserted\n",
    "plt.plot(df['rand'], color='purple') - plot number 1\n",
    "*That parameter increases numerically throughout the grid, so this only works for subplots with less than 10 figures (though there are other ways to handle such cases).\n",
    "*You could also add different types of plots as subplots, so we could have one plot in this figure be a scatter while the other is a line plot.\n",
    "*plt.tight_layout(),creates additional space arround the subplot to aid visibilty\n",
    "*f, ax = plt.subplots(figsize=(12, 9)), setting up subplot figure\n",
    "\n",
    "HISTOGRAM\n",
    "\n",
    "*It starts by dividing the values in the input into bins. These bins are evenly sized ranges that have an upper and lower bound. The histogram then counts how many values are in each bin and plots a bar for that count for each bin.\n",
    "*x = np.random.normal(10, 5, 1000), plt.hist(x), plt.show()\n",
    "*plt.hist(x, bins=40, color='red'), set the bin value for finer plot. Default bin is 10\n",
    "*plt.title('Default Bin Placement Demo'), set title\n",
    "*plt.xlabel('Random Values')\n",
    "*plt.show()\n",
    "*plt.hist(x, color='blue', bins=np.arange(-10, 40), alpha=.5), plt.hist(y, color='red', bins=np.arange(-10, 40), alpha=.5)to set the bin's positions. alpha sets the opacity(tranparency, if two histogrames overlap)\n",
    "*plt.hist(x, normed=True, color='blue', bins=np.arange(-10, 40), alpha=.5), setting normilazation to compare two or more hists\n",
    "\n",
    "*works.artist.value_counts().head(20).plot(kind='bar') - value count and plot\n",
    "*works.artist.value_counts().head(50)[1:51].plot(kind='bar', figsize=(10,5))\n",
    "*locations = artists.placeOfBirth.str.split(',', 1).tolist() - split by comma\n",
    "\n",
    "BOXPLOT\n",
    "\n",
    "*plt.boxplot(x), can include options as per histogram\n",
    "\n",
    "# Set up the matplotlib figure.\n",
    "*f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM DATA GENERATION HINTS\n",
    "\n",
    "*np.random.seed(1221), Make the random function consistent and replicable.\n",
    "\n",
    "*df['rand'] = np.random.rand(100), random numbers between 0 and 1\n",
    "\n",
    "*x = np.random.normal(10, 5, 1000), 10 = center 5 = standard deviation, 1000 = n\n",
    "\n",
    "*df = pd.DataFrame({'rand': np.random.normal(0, 5, 1000)}), generates a normally distributed data with mean = 0, sd = 5, put it in column 'rand' of df. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE FOR PREP COURSE\n",
    "\n",
    "https://github.com/Thinkful-Ed/data-201-resources/blob/master/data-sources.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS\n",
    "\n",
    "*The central tendency describes a point around which datapoints in a variable cluster. Measure one variable at a time (like one column in a df. The most common measures are; mean the median, and the mode.\n",
    "\n",
    "*MEAN; np.mean(df['age']), draw back: its sensitive to extreme values\n",
    "\n",
    "*MEDIAN: np.median(df['age'])\n",
    "\n",
    "*MODE: ind = np.argmax(counts), rep the index of the the mode\n",
    "values[ind], reo the most frequent element its self\n",
    "(values, counts) = np.unique(df['age'], return_counts=True), generates a turple of elements and their freq\n",
    "\n",
    "*BIAS/UNBIAS: \n",
    "#The mean, median and mode calculated from a sample are considered unbiased estimates of the population mean, median and mode. An estimate is \"unbiased\" if, across multiple representative samples, the sample estimates converge on the population value. A \"biased\" estimate would converge on a value that was either higher or lower than the population value.Unbiased estimates are useful because they let us use a small group of observations to make generalizations about a much larger group.\n",
    "\n",
    "*VARIANCE\n",
    "The variance of a variable describes how much values differ from the central tendency, and how much they differ from each other. If variance is low and most datapoints are similar to the central tendency, then each individual datapoint provides little new information about the concept being measured. If variance is high, then each individual datapoint is more likely to provide unique information about the concept being measured. While some people tend to be more interested in measures of central tendency like the mean, data scientists are usually just as excited about the variance. This is because data scientists generally want to answer questions about why things are different from each other; [v = sum((x - mean) ** 2) / (n - 1)] Estimates of sample variance divide by n - 1 because dividing by n would underestimate the population variance, creating bias. We'll cover bias in more depth later. df['column'].var, np.var(df.age) - with np\n",
    "\n",
    "*STANDARD DEVIATION\n",
    "The most common estimate of variability used by statisticians is the square root of the variance (s = v**0.5), called the standard deviation. np.std() - with np. NumPy gives us the useful np.std() function for working with standard deviations. A tricky default in numpy is to calculate the population standard deviation, dividing by n, rather than the sample standard deviation, dividing by n - 1. To calculate the sample instead of the population standard deviation we need to manually set the \"delta degrees of freedom\" with the ddof named parameter: np.std(df['age'], ddof=1) \n",
    "\n",
    "*STANDARD ERROR\n",
    "While the standard deviation tells us about variance in the population, the standard error tells us about the precision of our sample mean estimate. One example of standard errors at work is poll results, where they are called the \"margin of error\". For example, a poll might report that 44% of respondents were in favor of measure X, with a margin of error (standard error) of 3%. In other words, if the poll were run over and over again with new samples of respondents, the average response would fall between 41% (44-3) and 47% (44+3). Smaller standard errors mean more precise estimates.The formula for the standard error se of the mean is the standard deviation of the sample s divided by the square root of the sample size n.[se = s / (n ** 0.5)]. np.std(df['age'] ,ddof=1) / np.sqrt(len(df['age'])) - using np\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBABILITY\n",
    "\n",
    "*The [frequentist] school of thought defines probability as describing how often a particular outcome would occur in an experiment if that experiment were repeated over and over.\n",
    "\n",
    "*The [Bayesian] school of thought defines probability as describing how likely an observer expects a particular outcome to be in the future, based on previous experience and expert knowledge\n",
    "\n",
    "*Systematic differences between the sample and the population are known as selection bias.\n",
    "\n",
    "*The probability of two conditional events can be calculated by multiplying the probability of event A by the probability of event B conditioned on A. P(A ∩ B) = P(A) * P(B | A)\n",
    "\n",
    "*Baye's rule: P(A | B) = P(B | A) * P(A) / P(B), P(x/y) means the prob of x given y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA QUALITY\n",
    "\n",
    "*A data is skewed when the data is collected in an exceptional circumstance.\n",
    "\n",
    "NORMALITY\n",
    "\n",
    "*Data is described as \"normal,\" or \"normally distributed,\" if most values cluster in the center of the range, with the rest tapering off symmetrically to the left and the right. The mean and median of a normally distributed variable are equal.The information in a normal distribution can be summarized by the mean μ (\"mu\") and standard deviation σ (\"sigma\"). The probability density function for a normally distributed variable is in [Prep course 3.3.1]. Approximately 68% of the values in a normally-distributed variable fall within 1 standard deviation above or below the mean, 95% of values fall within two standard deviations of the mean, and 99.7% of values fall within three standard deviations of the mean. The normal distribution is useful for data scientists because:\n",
    "\n",
    "*It is easily summarized using just two statistics (mean and standard deviation).\n",
    "*The area under the curve is 1, making it easy to calculate the probability of individual outcomes within the distribution.\n",
    "*It describes many natural phenomena, such as blood pressure, height, weight, etc\n",
    "*In general, any variable that measures an outcome produced by many small effects acting additively and independently will have a close to normal distribution.\n",
    "*Lots of common scores (percentiles, z-scores) and statistical tests (t-tests, ANOVAs, bell-curve grading) assume a normal distribution.\n",
    "\n",
    "*the best method of deciding if your data is normal is to inspect the data visually using histograms and quantile-quantile (QQ) plots. QQ plots graph a variable with an unknown distribution against a variable with a known distribution. Values for each variable are sorted into ascending order, then plotted against each other with the known variable as the x-axis and the unknown variable as the y-axis. If the mystery variable shares the same distribution as the known variable, the result should be a straight line running from the lower left-hand corner to the upper right-hand corner of the plot. Deviations from the straight line indicate that the data does not fit the distribution.\n",
    "\n",
    "*Making two variables.\n",
    "rand1 = np.random.normal(50, 300, 1000)\n",
    "rand2 = np.random.poisson(1, 1000)\n",
    "\n",
    "*Sorting the values in ascending order.\n",
    "rand1.sort()\n",
    "rand2.sort()\n",
    "\n",
    "*Making a standard normally distributed variable with 1000 observations,\n",
    "a mean of 0, and standard deviation of 1 that we will use as our “comparison.”\n",
    "norm = np.random.normal(0, 1, 1000)\n",
    "\n",
    "*Sorting the values in ascending order.\n",
    "norm.sort()\n",
    "\n",
    "*plt.plot(norm, rand1, \"o\") \n",
    "\n",
    "*# Add a vertical line at the mean.\n",
    "plt.axvline(rand1.mean(), color='b', linestyle='solid', linewidth=2)\n",
    "\n",
    "*# Add a vertical line at one standard deviation above the mean.\n",
    "plt.axvline(rand1.mean() + rand1.std(), color='b', linestyle='dashed', linewidth=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA BASE - SQL\n",
    "\n",
    "*A database typically consists of a series of tables, each table with its own defined schema (or organization) and a number of records, or rows. Table data is organized first into columns. The term schema refers to a particular table's configuration of columns. Each record (or row) in a table must conform to the table schema. Ideally, all rows of data in a table would have an entry in every field, or column, though in the real world NULL, blank fields, and N/As (not available) abound.\n",
    "\n",
    "*There are three basic types of tables to be aware of: [Raw tables] - close to as entered by users, [processed tables] - proccessed into readable and usable format, [roll-up tables] - a specific kind of processed table, which take data and aggregate it.\n",
    "\n",
    "*relational databases  - databases based on the tables, columns, and rows, where tables can reference one another\n",
    "\n",
    "POSTGRESQL\n",
    "\n",
    "*CREATE TABLE table_name (\n",
    " some_column_name TYPE column_constraint,\n",
    " some_other_column_name TYPE,\n",
    " yet_another_column_name TYPE\n",
    "); - syntax for creating a new table in a DB, best practice is to use lowercase for table names. After the tablename, column are also named (with lowercase as well) and their types(e.g INT or TEXT) are specified. Among other optional arguments, constraint can also be added to columns such as prohibiting null values.\n",
    "\n",
    "CREATE TABLE percentage_15_19 (country text, Y1950 float, Y1955 float, Y1960 float, Y1965 float, Y1970 float, Y1975 float,\n",
    "\t\t\t\t\t\t\t   Y1980 float, Y1985 float, Y1990 float, Y1995 float, Y2000 float, Y2005 float, Y2010 float,\n",
    "\t\t\t\t\t\t\t   Y2015 float, Y2020 float, Y2025 float, Y2030 float, Y2035 float, Y2040 float, Y2045 float, \n",
    "\t\t\t\t\t\t\t   Y2050 float)\n",
    "\n",
    "COMMANDS\n",
    "\n",
    "*to see current user and database; [select current_user;]\n",
    "\n",
    "*to select all the content of as table; run [SELECT * FROM table_name;] from the specified DB, to select a coloumn, [SELECT column_name1, column_name2 FROM weather;] alnative select statements: [SELECT name column_name FROM table_name;] or [SELECT name As column_name FROM table_name; - the column is alias to 'column_name' and can be referenced] [pg_ctl -D \"C:\\Program Files\\PostgreSQL\\11\\data\" restart], to start the server from cmd\n",
    "\n",
    "SELECT\n",
    "    trip_id,\n",
    "    start_date\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    bike_id = 27 AND\n",
    "    zip_code = 94107\n",
    "ORDER BY duration DESC;\n",
    "\n",
    "*the code above selects column 'trip_id' and 'start_date' from table 'trips', for only bike_id = 27 and zip_code = 94107. It then orders the result by duration in descending order.\n",
    "\n",
    "SELECT\n",
    "    trip_id,\n",
    "    start_date\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    bike_id = 27 AND\n",
    "    subscriber_type LIKE 'Customer'\n",
    "ORDER BY duration DESC;\n",
    "\n",
    "*here, subsciber_type is a string, hence the LIKE keyword.\n",
    "\n",
    "*strings in SQL are specified striclty by single quote, double quote will retur error.\n",
    "\n",
    "*Use two dashes(--) for single-line comments.\n",
    "\n",
    "SELECT\n",
    "    trip_id,\n",
    "    start_date\n",
    "FROM\n",
    "    trips\n",
    "WHERE \n",
    "    bike_id = 27 AND\n",
    "    subscriber_type LIKE 'Customer'\n",
    "ORDER BY duration DESC\n",
    "LIMIT 3;\n",
    "\n",
    "*the limit clause limite the output to 3\n",
    "\n",
    "SQL RECOMENDATIONS\n",
    "\n",
    "*Put each column name in a select clause on its own line, with one level of indentation from the preceding line.\n",
    "*Follow the same indentation logic for FROM, WHERE, and ORDER BY blocks, giving each element its own line.\n",
    "*Similarly, each clause gets its own line.\n",
    "*Use all caps for clauses, function names, and the like.\n",
    "*Use the actual case of the column/table name when referring to column and table names.\n",
    "*Be consistent in your own use of casing, but recognize that SQL is not case sensitive, and it doesn't actually care about tabs and newlines.\n",
    "\n",
    " \n",
    "    \n",
    "*Group by clause. This rturns all the uniques cities in column city, no repitition \n",
    "\n",
    "SELECT\n",
    "    city\n",
    " FROM\n",
    "    stations\n",
    "GROUP BY city;\n",
    "\n",
    "\n",
    "AGGREGATE FUNCTIONS\n",
    "\n",
    "SELECT\n",
    "    city,\n",
    "    AVG(lat) as latitude,\n",
    "    AVG(long) as longitude,\n",
    "    COUNT(*) as station_count\n",
    "FROM\n",
    "    stations\n",
    "GROUP by 1;\n",
    "\n",
    "*this groups all the same cities in the city column together, takes ave of their long/lat and also give their counts. Note: if there are multiple selected column, also the selected coloumns must appear in the group statement.\n",
    "\n",
    "COMBINING TABLES\n",
    "\n",
    "SELECT\n",
    "    trips.trip_id,\n",
    "    trips.start_station,\n",
    "    stations.lat,\n",
    "    stations.long\n",
    "FROM\n",
    "    trips \n",
    "JOIN\n",
    "    stations\n",
    "ON\n",
    "    trips.start_station = stations.name;\n",
    "    \n",
    "*this returns four columns, from table trips and stations. [trips.start_station = stations.name]: means rows are only rturned when these two match.The same query could be done with aliases as below;\n",
    "\n",
    "SELECT\n",
    "    t.trip_id,\n",
    "    t.start_station,\n",
    "    s.lat,\n",
    "    s.long\n",
    "FROM\n",
    "    trips t\n",
    "JOIN\n",
    "    stations s\n",
    "ON\n",
    "    t.start_station = s.name;\n",
    "    \n",
    "*all the joins above have been default to inner joint which means that the only rows returned are the ones where there is both a match on the left table and a match on the right table. Because rows are only returned when there is a match on both sides, it doesn't matter which table is on the left and which table is on the right.Other forms of joins are: left out join [LEFT OUTER JOIN or just LEFT JOIN], right outer join [RIGHT OUTER JOIN or RIGHT JOIN], full outer join, returns matching from both left and right tables (returns large data can even choke a server.\n",
    "\n",
    "\n",
    "CASE\n",
    "\n",
    "*CASE statements allow you to set up conditions and then take action in a column based on them, e.g \n",
    "SELECT\n",
    "    (CASE WHEN dockcount > 20 THEN 'large' ELSE 'small' END) station_size,\n",
    "    COUNT(*) as station_count\n",
    "FROM \n",
    "    stations\n",
    "GROUP BY 1;\n",
    "\n",
    "*This CASE statement looks at the stations table and labels each row either 'large' or 'small' depending on the value of dockcount for that row, and then counts how many rows there are for each case. The group by statement makes it so we are counting based on the station size.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CREATE TABLE listings (\n",
    "    id bigint,\n",
    "    name text,\n",
    "    host_id bigint,\n",
    "    host_name text,\n",
    "    neighbourhood_group text,\n",
    "    neighbourhood text,\n",
    "    latitude numeric,\n",
    "    longitude numeric,\n",
    "    room_type text,\n",
    "    price int,\n",
    "    minimum_nights int,\n",
    "    number_of_reviews int,\n",
    "    last_review date,\n",
    "    reviews_per_month float,\n",
    "    calculated_host_listings_count int,\n",
    "    availability_365 int\n",
    ")\n",
    "\n",
    "COPY\n",
    "    (id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365)\n",
    "FROM\n",
    "    'C:\\Users\\user\\thinkful\\airBandB_data\\listings.csv' CSV HEADER DELIMITER ',';\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINKS\n",
    "\n",
    "David Odgers - QA\n",
    "https://stackoverflow.com/questions/13046284/how-to-log-in-as-postgres-user-with-postgresql-8-4-7-on-windows-7\n",
    "\n",
    "\n",
    "https://dba.stackexchange.com/questions/29767/permission-denied-in-file-trying-to-import\n",
    "\n",
    "https://www.virtualbox.org/\n",
    "\n",
    "https://www.ubuntu.com/download\n",
    "\n",
    "\n",
    "C:/Program Files/PostgreSQL/11/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN AND FURTHER PLOTS\n",
    "\n",
    "    #select rows 0-6 and all col of foods and print the top 20\n",
    "    print(foods.iloc[0:6,:].head(n=20))\n",
    "    \n",
    "    #plots rows 0-6 of col1    \n",
    "    plt.plot(foods.iloc[0:6]['col1'])\n",
    "    plt.scatter(foods['col2'],foods['col1']), plots scatter of col 1 and 2\n",
    "\n",
    "    #grouping and selecting data from a larger dataset: selects onle male from col 'sex', and lunch from 'time'\n",
    "    menlunchtip = tips.loc[(tips['sex']=='Male')&(tips['time']=='Lunch'),'tip']\n",
    "\n",
    "#QQ PLOT\n",
    "\n",
    "    print('QQ plots show how close a variable is to known distribution, and any outliers.')\n",
    "    #mean is 0, SD is 1\n",
    "    norm= np.random.normal(0, 1, 140)\n",
    "    norm.sort()\n",
    "    plt.plot(norm, foods['col1'].sort_values(), \"o\")\n",
    "    \n",
    "#BOX PLOT\n",
    "\n",
    "    print('Boxplots are used to compare groups and to identify differences in variance, as well as outliers.')\n",
    "    plt.boxplot([foods[foods['index']=='g1']['col1'],foods[foods['index']=='g2']['col1']])\n",
    "    plt.show()\n",
    "\n",
    "#IMPORTING SEABORN\n",
    "\n",
    "    import seaborn as sns\n",
    "    #loading data in seaborn\n",
    "    tips = sns.load_dataset(\"tips\")\n",
    "    \n",
    "    # Setting the overall aesthetic for seaborn plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    #seaborn box plot, hue categorizes the legend\n",
    "    ax = sns.boxplot(x='sex',y='tip',hue='time',data=tips,palette='pastel')\n",
    "    \n",
    "#SNS BARPLOT OR DYNAMITE PLOT\n",
    "    #it kinda plot the counts in each category and shows the margin of erro as well.\n",
    "\n",
    "    g = sns.factorplot(x=\"sex\", y=\"tip\", hue=\"time\", data=tips,\n",
    "                   size=6, kind=\"bar\", palette=\"pastel\", ci=95)\n",
    "    #point plot, more efficient than barplot\n",
    "    g = sns.factorplot(x=\"sex\", y=\"tip\", hue=\"time\", data=tips,\n",
    "                   size=6, kind=\"point\", palette=\"pastel\",ci=95,dodge=True,join=False)\n",
    "    \n",
    "    #to declear a df for scatter plot matrix\n",
    "    g = sns.PairGrid(df, diag_sharey=False)\n",
    "    \n",
    "    #make scatter plots\n",
    "    g.map_upper(plt.scatter, alpha=.5)\n",
    "    \n",
    "    # Draw the heatmap using seaborn, to visualize data correlations\n",
    "    sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "    plt.show()\n",
    "    \n",
    "    # countplot, lengent in below is based on gender. plotting the count of gender in each sch. base color is green\n",
    "    sns.countplot(y=\"school_name\", hue=\"gender\", data=df, palette=\"Greens_d\")\n",
    "    \n",
    "    #Set the default plot aesthetics to be prettier. \n",
    "    **sns.set_style(\"white\")**\n",
    "    \n",
    "    #draw heatmap\n",
    "    sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3     True\n",
       "4    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MANIPULATING STRINGS IN PYTHON\n",
    "\n",
    "sample_string = 'take 5'\n",
    "#returns True. Similar methods: .isalpha(), .isnumeric(), .isspace(), .isalnum()\n",
    "print(sample_string[5].isdigit())\n",
    "\n",
    "#APPLY METHOD, allows us apply a function to all the elements in list or string. isdigit() can be applied to a list, series or string throguh the apply method\n",
    "money = pd.Series([400, 111, '$20', 57, 'Lots'])\n",
    "\n",
    "def is_a_string(item):\n",
    "    return str(item).isdigit()\n",
    "#we can now apply our function to every element of money withount a loop\n",
    "money.apply(is_a_string)\n",
    "\n",
    "#LAMBDA METHOD, simplifies the apply method. It allows the definition of an anonymous method. in below example lambda takes \n",
    "#element x, turns it to a string and apply the isdigit method to the string. x is the element of money\n",
    "\n",
    "print(money.apply(lambda x: str(x).isdigit()))\n",
    "\n",
    "#FILTER, takes a function that returns boolean and a list or series, normally it should return iterators for True. But if used\n",
    "#with list as below, it returns values\n",
    "print(list(filter(lambda x: str(x).isdigit(), money)))\n",
    "\n",
    "#applying filter to iterate through each element of the series and join the true one\n",
    "print(money.apply(lambda x: ''.join(list(filter(str.isdigit, str(x))))))\n",
    "\n",
    "#THE SPLIT METHOD, allows splitting withiug defining a lambda fn\n",
    "\n",
    "words = pd.Series([\n",
    "    'MollyMalone$molmal@gmail.com',\n",
    "    'JeffreyJones$jefjo@hotmail.com',\n",
    "    'DeadParrot$fjords@gmail.com'\n",
    "])\n",
    "\n",
    "word_split = words.str.split('$', expand=True)\n",
    "names = word_split[0]\n",
    "emails = word_split[1]\n",
    "print(names, '\\n')\n",
    "print(emails)\n",
    "\n",
    "print(names.str.split('[A-Z]', expand=True)), using capital letter to split\n",
    "#the issue with the above is the capital character will be cutoff. this is better done with re\n",
    "import re\n",
    "firstname = names.apply(lambda x: re.findall('[A-Z][a-z]*', x)[0])\n",
    "lastname = names.apply(lambda x: re.findall('[A-Z][a-z]*', x)[1])\n",
    "\n",
    "#STRING REPLACEMENT,Pandas\n",
    "\n",
    "print(emails.str.replace('@', ' at '),'\\n')\n",
    "\n",
    "#STRIP WHITE SPACE\n",
    "\n",
    "words = pd.Series([' duck', 'duck ', ' duck ', 'goose'])\n",
    "stripped = words.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DATA\n",
    "\n",
    "data = {\n",
    "    'age': [27, 50, 34, None, None, None],\n",
    "    'gender': ['f', 'f', 'f', 'm', 'm', None],\n",
    "    'height' : [64, None, 71, 66, 68, None],\n",
    "    'weight' : [140, None, 130, 110, 160, None],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Full dataset.\n",
    "print(df)\n",
    "\n",
    "# Drop all rows that have any missing values in any column.\n",
    "print(df.dropna())\n",
    "\n",
    "# Drop only rows where all values are missing.\n",
    "print(df.dropna(how='all'))\n",
    "\n",
    "# Drop only rows where more than two values are missing.\n",
    "print(df.dropna(thresh=2))\n",
    "\n",
    "# Drop all rows that have any missing values in the 'gender' or 'height' columns.\n",
    "print(df.dropna(subset=['gender','height']))\n",
    "\n",
    "\n",
    "#INPUTING DATA\n",
    "\n",
    "#The most straightforward involves replacing missing values with the mode, mean, or median of the variable. \n",
    "#This method isn't perfect: it keeps central tendency the same, but reduces variance and correlations among variables\n",
    "\n",
    "# For each numeric column, replace the missing values with the mean for that column.\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "print(df)\n",
    "\n",
    "# For each column, replace the missing values with the most common value for that\n",
    "# column. Useful for filling in missing categorical values.\n",
    "# As written, this command will fill in missing values for both numerical and\n",
    "# categorical columns.\n",
    "df = pd.DataFrame(data)\n",
    "df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "print(df)\n",
    "\n",
    "#If you'd like to see a more sophisticated method of replacing missing data, involving grouping existing entries into \"similar\" groups and filling in the missing values within a group with the mean for that group, check out this in-depth tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENTAL DESIGN\n",
    "\n",
    "#----A/B TESTING------\n",
    "\n",
    "#1. A/B testing is a technique used to identify whether one version of an object of interest (product, marketing campaign, email text) is better at producing a desired outcome than another.\n",
    "#components of A/B testing; control version(already existing), test version(has some changes from the control,often called treatment)\n",
    "#If starting from scratch, however, you may have two different test versions to compare to each other.\n",
    "\n",
    "#2. A sample, divided into two groups.Each sample should be selected so that it is similar to the population you want to \n",
    "#understand. The groups should be similar to one another so that any differences between them can be attributed to seeing \n",
    "#version A or version B and not something else. You also want the split of between A and B to be as random as possible.\n",
    "\n",
    "#3. A hypothesis. Your hypothesis is what you expect to happen. For example, \"I expect the HTML email will achieve a higher \n",
    "#open and conversion rate than the plain text email.\"\n",
    "\n",
    "#-------T-TEST--------\n",
    "\n",
    "#see formular in unit 1 lesson\n",
    "\n",
    "# if 2 groups have different means say: 20 and 35. To know how meaningfull this difference is, we need to calculate the\n",
    "# difference in means through a t-test using the means, amount of variability(SD) and N.The more noisy the groups are \n",
    "#(higher sds), the larger a difference between them must be for us to be confident it is a real difference and not the result\n",
    "#of the noise. On the other hand, larger groups (higher Ns) give us more accurate means (means that are more likely to represent\n",
    "#the population), reducing noise and letting us detect smaller group differences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Markdown cheatsheet\n",
    "#### Running a markdown; Ctrl + Enter or shft + Enter\n",
    "#### String\n",
    "manual line break; <br>\n",
    "Bold; __string__ or **string** <br>\n",
    "Italic; _string_ or *string* <br>\n",
    "maths simbol; $ √9 = 3 $ <br>\n",
    "color; <font color = red> enter your text here</font> <br>\n",
    "> type text here to create indent, everything is indented until a carraige return <br>\n",
    "\n",
    "- for a circular bullet <br> \n",
    "\n",
    "1. for number list <br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 2; Supervised Learning <br>\n",
    "\n",
    "_In supervised learning, models will be __trained__ to generate a certain outcome or dependent variable._ <br>\n",
    "\n",
    "_**Statistical model:** a simplified mathematical representation of the data scientist's best guess about the underlying processes that created the data._ <br>\n",
    "\n",
    "_**Scatter plot:** represents the relationship between two continuous variables. <br>\n",
    "\n",
    "_**Correlation coefficient r:** is a numeric representation of the linear relationship between two continuous variables. <br>\n",
    "\n",
    "_**Correlations range from:** -1 (as values in one variable go up, values in the other go down) to 0 (no relationship between the variables) to 1 (as values in one variable go up, values in the other go up as well).<br>\n",
    "\n",
    "_**correlation matrix** shows the correlation coefficients between variable in a data: corrmat = df.corr() <br> \n",
    "\n",
    "### For a continuous and a categorical pair:<br>\n",
    "_Box, violin, or similar plots estimate the value of the continuous variable for each value of the categorical variable.<br>\n",
    "_Descriptive statistics, including estimates of central tendency and variance, for the continuous variable at each level of the categorical variable.<br>\n",
    "_T-tests and ANOVA <br>\n",
    "_Look for: Outliers in each group, very small groups <br>\n",
    "\n",
    "_**PairGrid:** for ploting pairwise relationship in a dataset, g = sns.PairGrid(df, diag_sharey=False), see other options in doc. doc:https://seaborn.pydata.org/generated/seaborn.PairGrid.html <br>\n",
    "\n",
    "### PCA <br> \n",
    "_is a complexity-reduction technique that tries to reduce a set of variables down to a smaller set of components that represent most of the information in the variables.<br>\n",
    "\n",
    "_**PCA** works by identifying sets of variables that share variance, and creating a component to represent that variance.<br>\n",
    "\n",
    "_**PCA** works best for normally-distributed data and assumes the relationships among variables are linear.<br>\n",
    "\n",
    "### Normalizing data <br>\n",
    "_means making all variables(cols) have a mean of 0 and standard deviation of 1; X = StandardScaler().fit_transform(df) <br>\n",
    "\n",
    "### Covariance <br>\n",
    "_sum of the products of the mean deviations(value - mean) of each data points of the two distriubtion divided by N <br>\n",
    "\n",
    "_Calculating eigenvalues and eigenvectors: eig_val_cov, eig_vec_cov = np.linalg.eig(Cx) <br>\n",
    "\n",
    "_The one thing all feature selection algorithms have in common is that they work better when data is separated into a training set and a test set, and feature selection is run on the training set. <br>\n",
    "\n",
    "### Feature selection algorithms fall into three broad groups:\n",
    "_**Filter methods** evaluate each feature separately and assign it a \"score\" that is used to rank the features, with scores above a certain cutoff point being retained or discarded.\n",
    "_**Wrapper methods** select sets of features. Different sets are constructed, evaluated in terms of their predictive power in a model, and performance is compared to the performance of other sets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling variables\n",
    "_Just in case some of the variables are ondifferent scales than others (one ranges from 0 to 100, one ranges from 0 to7, for example) we scale them before averaging by subtracting the average of each variable from all values in that variable, then dividing by the standard deviation.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four panel plot sample\n",
    "fig = plt.figure()\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plt.hist(df['agea'].dropna())\n",
    "plt.title('Raw')\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plt.hist(np.log(df['agea'].dropna()))\n",
    "plt.title('Log')\n",
    "\n",
    "fig.add_subplot(223)\n",
    "plt.hist(np.sqrt(df['agea'].dropna()))\n",
    "plt.title('Square root')\n",
    "\n",
    "ax3=fig.add_subplot(224)\n",
    "plt.hist(1/df['agea'].dropna())\n",
    "plt.title('Inverse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "_**PCA** works best for normally distributed data and assumes that variable have linear relationships.<br>\n",
    "_**PCA** works best when variables range from weakly correlated to strongly correllated.<br>\n",
    "_in **PCA** variable are first standardized to have mean of 0 and standard deviation of 1, this ensures that all variable go through the origin point and share the same variance. Next, the axes of the n-dimensional space are rotated to minimize the distance between the datapoints and the axes. During rotation, some axes become shorter, indicating that variance along that axis is small and the axis contains little information. That axis, and the principal component it represents, can be dropped without much loss of information.\n",
    "\n",
    "##### make scater plot with sns\n",
    "t = sns.regplot(\n",
    "    'columnx',\n",
    "    'columny',\n",
    "    data,\n",
    "    x_jitter=.49,\n",
    "    y_jitter=.49,\n",
    "    fit_reg=False\n",
    ")\n",
    "\n",
    "_**other settings for the plot**\n",
    "t.set(xlim=(-1, 11), ylim=(-1, 11)) limiting the axis range\n",
    "t.axhline(0, color='k', linestyle='-', linewidth=2)\n",
    "t.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "t.axes.set_title('Raw data')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
